{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Application of machine learning in healthcare**","metadata":{"id":"yEJcbJ5xSWVT"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, average_precision_score, classification_report\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Load data (assumes a CSV with features and a 'readmitted' binary target)\ndf = pd.read_csv('/kaggle/input/ecr-healthcare/ehr_readmission.csv')\n\n# Define static and dynamic feature lists (example)\nstatic_features = ['age', 'gender', 'race', 'admission_type']\ncontinuous_features = ['lab_creatinine', 'lab_glucose', 'vital_hr', 'vital_bp_systolic']\ncategorical_features = ['gender', 'race', 'admission_type']\ntarget = 'readmitted'\n\n# Split data\ndf_train, df_test = train_test_split(df, test_size=0.2, stratify=df[target], random_state=42)\nX_train = df_train[static_features + continuous_features]\nX_test = df_test[static_features + continuous_features]\ny_train = df_train[target].values\ny_test = df_test[target].values\n\n# Preprocessing pipeline\nnumeric_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n    ('num', numeric_pipeline, continuous_features),\n    ('cat', cat_pipeline, categorical_features)\n])\n\n# Logistic Regression baseline\nlogreg_pipeline = Pipeline([\n    ('preproc', preprocessor),\n    ('clf', LogisticRegression(max_iter=1000))\n])\n\nlogreg_pipeline.fit(X_train, y_train)\ny_pred_proba = logreg_pipeline.predict_proba(X_test)[:, 1]\nprint(\"Logistic Regression AUC-ROC:\", roc_auc_score(y_test, y_pred_proba))\nprint(\"Logistic Regression AUC-PR:\", average_precision_score(y_test, y_pred_proba))\n\n# Prepare data for PyTorch MLP\nX_train_proc = preprocessor.fit_transform(X_train)\nX_test_proc = preprocessor.transform(X_test)\n\nif hasattr(X_train_proc, 'toarray'):\n    X_train_proc = X_train_proc.toarray()\n    X_test_proc = X_test_proc.toarray()\n\ntensor_x_train = torch.tensor(X_train_proc, dtype=torch.float32)\ntensor_y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\ntensor_x_test = torch.tensor(X_test_proc, dtype=torch.float32)\ntensor_y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\ntrain_dataset = TensorDataset(tensor_x_train, tensor_y_train)\ntest_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n\ndataloader_train = DataLoader(train_dataset, batch_size=64, shuffle=True)\ndataloader_test = DataLoader(test_dataset, batch_size=64)\n\nclass ReadmissionMLP(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ReadmissionMLP(tensor_x_train.shape[1]).to(device)\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(1, 11):\n    model.train()\n    epoch_loss = 0\n    for X_batch, y_batch in dataloader_train:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    print(f\"Epoch {epoch}: Train loss = {epoch_loss / len(dataloader_train):.4f}\")\n\nmodel.eval()\nwith torch.no_grad():\n    preds = []\n    targets = []\n    for X_batch, y_batch in dataloader_test:\n        X_batch = X_batch.to(device)\n        y_pred = model(X_batch).cpu().numpy()\n        preds.extend(y_pred.flatten().tolist())\n        targets.extend(y_batch.numpy().flatten().tolist())\n\nprint(\"MLP AUC-ROC:\", roc_auc_score(targets, preds))\nprint(\"MLP AUC-PR:\", average_precision_score(targets, preds))\nprint(classification_report(targets, [1 if p > 0.5 else 0 for p in preds], zero_division=0))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jogtSwGtTSFV","outputId":"55ebafff-9a49-49fc-c552-3900c707f6ba","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T13:20:23.643730Z","iopub.execute_input":"2025-04-20T13:20:23.644738Z","iopub.status.idle":"2025-04-20T13:20:27.588795Z","shell.execute_reply.started":"2025-04-20T13:20:23.644706Z","shell.execute_reply":"2025-04-20T13:20:27.587740Z"}},"outputs":[{"name":"stdout","text":"Logistic Regression AUC-ROC: 0.5227373722411495\nLogistic Regression AUC-PR: 0.21483853220711757\nEpoch 1: Train loss = 0.6189\nEpoch 2: Train loss = 0.5302\nEpoch 3: Train loss = 0.5395\nEpoch 4: Train loss = 0.5231\nEpoch 5: Train loss = 0.5248\nEpoch 6: Train loss = 0.5205\nEpoch 7: Train loss = 0.5198\nEpoch 8: Train loss = 0.5241\nEpoch 9: Train loss = 0.5223\nEpoch 10: Train loss = 0.5264\nMLP AUC-ROC: 0.5288105465856909\nMLP AUC-PR: 0.2181835338355407\n              precision    recall  f1-score   support\n\n         0.0       0.79      1.00      0.88       157\n         1.0       0.00      0.00      0.00        43\n\n    accuracy                           0.79       200\n   macro avg       0.39      0.50      0.44       200\nweighted avg       0.62      0.79      0.69       200\n\n","output_type":"stream"}],"execution_count":2}]}