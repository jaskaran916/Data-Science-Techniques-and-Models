{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10061972,"sourceType":"datasetVersion","datasetId":6200809}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaskarandhillon1609/supervised-learning-techniques?scriptVersionId=210535770\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Experiment 1: Handling Missing Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/supervised-learning-datasets/data.csv')\n\n# Identify columns with missing values and calculate the percentage of missing data\nmissing_percentage = df.isnull().mean() * 100\nprint(missing_percentage)\n\n# Replace missing values in numerical columns with their respective mean\nfor column in df.select_dtypes(include=['float64', 'int64']).columns:\n    df[column].fillna(df[column].mean(), inplace=True)\n\n# Replace missing values in categorical columns with the most frequent value\nfor column in df.select_dtypes(include=['object']).columns:\n    df.fillna({column: df[column].mode()[0]}, inplace=True)\n\n# Drop rows with more than 50% missing values\ndf.dropna(thresh=len(df.columns) * 0.5, inplace=True)\n\n# Verify that all missing data issues have been resolved\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:39:33.739883Z","iopub.execute_input":"2024-11-30T21:39:33.740431Z","iopub.status.idle":"2024-11-30T21:39:33.793211Z","shell.execute_reply.started":"2024-11-30T21:39:33.740383Z","shell.execute_reply":"2024-11-30T21:39:33.792232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 2: Removing Duplicates","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/supervised-learning-datasets/data.csv')\n\n# Check for duplicate rows\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\nprint(df[duplicates].head())\n\n# Remove duplicate rows\ndf_cleaned = df.drop_duplicates()\n\n# Reset index\ndf_cleaned.reset_index(drop=True, inplace=True)\nprint(df_cleaned.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:39:49.533435Z","iopub.execute_input":"2024-11-30T21:39:49.533861Z","iopub.status.idle":"2024-11-30T21:39:49.562802Z","shell.execute_reply.started":"2024-11-30T21:39:49.533813Z","shell.execute_reply":"2024-11-30T21:39:49.561741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 3: Data Transformation and Normalization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\n\n# Create a DataFrame from the Iris dataset\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Check the first few rows of the dataset\nprint(df.head())\n\n# Standardize numerical columns\nnumerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\nscaler = StandardScaler()\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n\n# Normalize numerical columns to range between 0 and 1\nmin_max_scaler = MinMaxScaler()\ndf[numerical_cols] = min_max_scaler.fit_transform(df[numerical_cols])\n\n# The Iris dataset does not have a 'total sulfur dioxide' column, so this part can be removed or modified.\n# If you want to apply log transformation, you can apply it to a valid numerical column.\n# For example, applying it to the first numerical column (sepal length):\ndf[df.columns[0]] = np.log(df[df.columns[0]] + 1)  # Adding 1 to avoid log(0)\n\n# Convert a categorical column into dummy/one-hot encoded variables\n# The Iris dataset does not have categorical variables, so this step may not apply directly.\n# However, if you had a categorical variable, you could do:\n# df = pd.get_dummies(df, columns=['categorical_column'])\n\n# Display the transformed dataset\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:39:52.795274Z","iopub.execute_input":"2024-11-30T21:39:52.795696Z","iopub.status.idle":"2024-11-30T21:39:53.520283Z","shell.execute_reply.started":"2024-11-30T21:39:52.795637Z","shell.execute_reply":"2024-11-30T21:39:53.51919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 4: Detecting and Handling Outliers","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the Wine Quality dataset\ndf = pd.read_csv('/kaggle/input/supervised-learning-datasets/data.csv', delimiter=';')  # Update with your actual path\n\n# Display the first few rows and the columns to understand the data\nprint(df.head())\nprint(df.columns.tolist())  # Print the list of column names\n\n\n# Choose the correct numerical column to analyze for outliers\ncolumn_name = 'fixed acidity'  # Ensure this matches the column name exactly\n\n# Check if the column exists in the DataFrame\nif column_name in df.columns:\n    # Identify outliers using IQR\n    Q1 = df[column_name].quantile(0.25)\n    Q3 = df[column_name].quantile(0.75)\n    IQR = Q3 - Q1\n\n    # Define outlier bounds\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n\n    # Identify outliers\n    outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n    print(\"Outliers:\")\n    print(outliers)\n\n    # Visualize outliers using a boxplot\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x=df[column_name])\n    plt.title(f'Boxplot of {column_name} to visualize outliers')\n    plt.show()\n\n    # Replace outliers with the 5th and 95th percentile values\n    df[column_name] = np.where(df[column_name] < lower_bound, df[column_name].quantile(0.05), df[column_name])\n    df[column_name] = np.where(df[column_name] > upper_bound, df[column_name].quantile(0.95), df[column_name])\n\n    # Display the transformed dataset\n    print(df.head())\nelse:\n    print(f\"Column '{column_name}' does not exist in the DataFrame.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:40:04.162237Z","iopub.execute_input":"2024-11-30T21:40:04.162852Z","iopub.status.idle":"2024-11-30T21:40:04.732473Z","shell.execute_reply.started":"2024-11-30T21:40:04.16281Z","shell.execute_reply":"2024-11-30T21:40:04.73122Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 5: Building a Simple Linear Regression Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/supervised-learning-datasets/ad.csv')\nprint(df.columns.tolist())\ndf.head()\n# Define dependent (Y) and independent (X) variables\nX = df[['TV']]\nY = df['Sales']\n\n# Plot the relationship\nplt.scatter(X, Y)\nplt.xlabel('TV')\nplt.ylabel('Sales')\nplt.show()\n\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X, Y)\n\n# Predictions\nY_pred = model.predict(X)\n\n# Evaluate the model\nmse = mean_squared_error(Y, Y_pred)\nr2 = r2_score(Y, Y_pred)\nprint(f'MSE: {mse}, R-squared: {r2}')\n\n# Visualize the regression line\nplt .plot(X, Y, 'o')\nplt.plot(X, Y_pred, color='red')\nplt.xlabel('TV')\nplt.ylabel('Sales')\nplt.title('Linear Regression Fit')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:40:26.432842Z","iopub.execute_input":"2024-11-30T21:40:26.43339Z","iopub.status.idle":"2024-11-30T21:40:27.177196Z","shell.execute_reply.started":"2024-11-30T21:40:26.433333Z","shell.execute_reply":"2024-11-30T21:40:27.176013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 6: Multiple Linear Regression","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'auto.csv' contains string data in the specified columns\ndf = pd.read_csv('/kaggle/input/supervised-learning-datasets/auto.csv')\n\nimport numpy as np\n\n# Replace '?' with NaN\ndf.replace('?', np.nan, inplace=True)\n\n# Convert columns to numeric, forcing errors to NaN\ndf['displacement'] = pd.to_numeric(df['displacement'], errors='coerce')\ndf['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')\ndf['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n\n# Optionally drop rows with NaN values\ndf.dropna(subset=['displacement', 'horsepower', 'weight'], inplace=True)\n\n# Now you can proceed with defining X and Y\nX = df[['displacement', 'horsepower', 'weight']]\nY = df['mpg']  # Ensure 'mpg' is also numeric and valid\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Fit a multiple linear regression model\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\n\n# Make predictions\nY_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(Y_test, Y_pred)\nr2 = r2_score(Y_test, Y_pred)\n\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:40:42.482198Z","iopub.execute_input":"2024-11-30T21:40:42.48259Z","iopub.status.idle":"2024-11-30T21:40:42.521429Z","shell.execute_reply.started":"2024-11-30T21:40:42.482555Z","shell.execute_reply":"2024-11-30T21:40:42.52009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 7: Binary Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/supervised-learning-datasets/diabetes.csv')\n\n# Define features and target variable\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:40:58.61784Z","iopub.execute_input":"2024-11-30T21:40:58.618257Z","iopub.status.idle":"2024-11-30T21:40:58.671958Z","shell.execute_reply.started":"2024-11-30T21:40:58.618218Z","shell.execute_reply":"2024-11-30T21:40:58.670072Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 8: Implementing k-NN for Classification","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the Iris dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['Species'] = iris.target  # Add the target variable\n\n# Define features and target variable\nX = df.drop('Species', axis=1)\ny = df['Species']\n\n# Normalize the features\nX = (X - X.mean()) / X.std()\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build a k-NN classifier\nk_values = [1, 3, 5, 10]\nfor k in k_values:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'Accuracy for k={k}: {accuracy}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:41:08.661396Z","iopub.execute_input":"2024-11-30T21:41:08.662442Z","iopub.status.idle":"2024-11-30T21:41:08.773697Z","shell.execute_reply.started":"2024-11-30T21:41:08.662394Z","shell.execute_reply":"2024-11-30T21:41:08.772397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 9: SVM for Binary Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\n# Load the dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['Species'] = iris.target  # Add the target variable\n\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train SVM with linear kernel\nmodel_linear = SVC(kernel='linear')\nmodel_linear.fit(X_train, y_train)\ny_pred_linear = model_linear.predict(X_test)\n\n# Train SVM with RBF kernel\nmodel_rbf = SVC(kernel='rbf')\nmodel_rbf.fit(X_train, y_train)\ny_pred_rbf = model_rbf.predict(X_test)\n\n# Evaluate the models\naccuracy_linear = accuracy_score(y_test, y_pred_linear)\naccuracy_rbf = accuracy_score(y_test, y_pred_rbf)\nprint(f'Accuracy (Linear Kernel): {accuracy_linear}, Accuracy (RBF Kernel): {accuracy_rbf}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:41:16.028012Z","iopub.execute_input":"2024-11-30T21:41:16.02851Z","iopub.status.idle":"2024-11-30T21:41:16.053087Z","shell.execute_reply.started":"2024-11-30T21:41:16.02846Z","shell.execute_reply":"2024-11-30T21:41:16.051883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Experiment 10: Dimensionality Reduction Using PCA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Standardize the data\nX_standardized = (df - df.mean()) / df.std()\n\n# Apply PCA\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X_standardized)\n\n# Visualize the results\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=data.target, cmap='viridis')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('PCA Result on Breast Cancer Dataset')\nplt.colorbar(label='Cancer Type')\nplt.show()\n\n# Analyze the variance explained by each principal component\nexplained_variance = pca.explained_variance_ratio_\nprint(f'Explained variance by each principal component: {explained_variance}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:46:32.530161Z","iopub.execute_input":"2024-11-30T21:46:32.530762Z","iopub.status.idle":"2024-11-30T21:46:32.935496Z","shell.execute_reply.started":"2024-11-30T21:46:32.530705Z","shell.execute_reply":"2024-11-30T21:46:32.9346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}